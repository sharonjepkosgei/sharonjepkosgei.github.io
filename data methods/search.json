[
  {
    "objectID": "posts/Project/project.html",
    "href": "posts/Project/project.html",
    "title": "Project",
    "section": "",
    "text": "Final Project Paper\nProject Presentation\nProject Proposal\nSentiment Analysis"
  },
  {
    "objectID": "posts/Assignment 7/EPPS_6302_07.html",
    "href": "posts/Assignment 7/EPPS_6302_07.html",
    "title": "Downloding Multiple files",
    "section": "",
    "text": "Code Here\n## Scraping Government data\n## Website: GovInfo (https://www.govinfo.gov/app/search/)\n## Prerequisite: Download from website the list of files to be downloaded\n## Designed for background job\n\n# Start with a clean plate and lean loading to save memory\n \ngc(reset=T)\nrm(list = ls())\n\n# install.packages(c(\"purrr\", \"magrittr\")\nlibrary(purrr)\nlibrary(magrittr)\n\n## Set path for reading the listing and home directory\n## For Windows, use \"c:\\\\directory\\\\subdirectory\"\nsetwd(\"c:\\\\\")\n\ngovfiles= read.csv(\"C:\\\\EPPS 6302\\\\govinfo-search-results-2023-11-07T22_18_19.csv\")\ngovfiles$id = govfiles$packageId\npdf_govfiles_url = govfiles$pdfLink\npdf_govfiles_id &lt;- govfiles$id\n\n# Directory to save the pdf's\nsave_dir &lt;- \"C:\\\\EPPS 6302\\\\pdf\\\\\"\n\n# Function to download pdfs\ndownload_govfiles_pdf &lt;- function(url, id) {\n  tryCatch({\n    destfile &lt;- paste0(save_dir, \"govfiles_\", id, \".pdf\")\n    download.file(url, destfile = destfile, mode = \"wb\") # Binary files\n    Sys.sleep(runif(1, 1, 3))  # Important: random sleep between 1 and 3 seconds to avoid suspicion of \"hacking\" the server\n    return(paste(\"Successfully downloaded:\", url))\n  },\n  error = function(e) {\n    return(paste(\"Failed to download:\", url))\n  })\n}\n\n# Download files, potentially in parallel for speed\n# Simple timer, can use package like tictoc\n#start.time &lt;- Sys.time()\n#message(\"Starting downloads\")\n#results &lt;- 1:length(pdf_govfiles_url) %&gt;% \n  #purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))\n#message(\"Finished downloads\")\n#end.time &lt;- Sys.time()\n#time.taken &lt;- end.time - start.time\n#time.taken\n\n# Print results\n#print(results)\n\n\n## Exercise: Try downloading 118th Congress Congressional Hearings in Committee on Foreign Affairs?"
  },
  {
    "objectID": "posts/Assignment 4/EPPS_6302_04.html",
    "href": "posts/Assignment 4/EPPS_6302_04.html",
    "title": "Word Cloud",
    "section": "",
    "text": "Code Here\n# Install the easypackages package \n#install.packages(\"easypackages\")\nlibrary(easypackages)\n\n# Load multiple packages using easypackage function \"packages\"\npackages(\"XML\",\"wordcloud\",\"RColorBrewer\",\"readr\",\"NLP\",\"tm\",\"quanteda\", prompt = T)\n\n\n# Read the comments from a CSV file\ndataset &lt;- read_csv(\"https://raw.githubusercontent.com/sharonjepkosgei/sharonjepkosgei.github.io/main/nbc_videos.csv\")\n#head(dataset)\n\n# Extract the 'description' column from the CSV as a character vector\nvid_des &lt;- dataset$description\n#head(vid_des)\n#vid_toks &lt;- tokens(vid_des)\n# Create a corpus from the video description\ncorpus &lt;- Corpus(VectorSource(vid_des))\n\n# remove punctuation\ncorpus &lt;- tm_map(corpus, removePunctuation)\n# remove numbers\ncorpus &lt;- tm_map(corpus, removeNumbers)\n# remove stopwords\ncorpus &lt;- tm_map(corpus, function(x) removeWords(x,stopwords(\"english\")))\n# Create Term Document Matrix\n\n# Create Term Document Matrix\n\nwords_to_remove &lt;- c(\"said\",\"from\",\"what\",\"told\",\"over\",\"more\",\"other\",\"have\",\"last\",\"with\",\"this\",\"that\",\"such\",\"when\",\"been\",\"says\",\"will\",\"also\",\"where\",\"why\",\"would\",\"today\", \"now\", \"two\")\ncorpus &lt;- tm_map(corpus, removeWords, words_to_remove)\n\ntdm &lt;- TermDocumentMatrix(corpus)\n#inspect(tdm)\n\nm &lt;- as.matrix(tdm)\nwordCounts &lt;- rowSums(m)\nwordCounts &lt;- sort(wordCounts, decreasing=TRUE)\n#head(wordCounts)\n\n\n# Create Wordcloud\ncloudFrame&lt;-data.frame(word=names(wordCounts),freq=wordCounts)\nset.seed(1234)\nwordcloud(cloudFrame$word,cloudFrame$freq)\n\n\n\n\n\n\n# Create a bar plot for the top words\ntop_words &lt;- head(wordCounts, 10)  \nbarplot(top_words, main=\"Top Words in Video Descriptions\", \n        ylab=\"Frequency\", col=\"darkslategrey\", las=2)\n\n\n\n\n\n#word cloud\n\n\nwordcloud(names(wordCounts),wordCounts, min.freq=5,random.order=FALSE, max.words=500,scale=c(2.5,0.5), rot.per=0.35,colors=brewer.pal(8,\"Dark2\"))"
  },
  {
    "objectID": "posts/Assignment 2/EPPS_6302_02.html",
    "href": "posts/Assignment 2/EPPS_6302_02.html",
    "title": "Panel Data and Google Trends",
    "section": "",
    "text": "Part A: Building a Qualtrics Panel\nAdding Panel:\n\nGo to Directories (Left-upper menu)\nThen Lists and create list.\n\n\n\nMy Panel\n\n\n\n\n\nPart B: Google Trends Data\n\n\nCode\n\ninstall.packages(“gtrendsR”)\nlibrary(gtrendsR)\nBidenTrumpElection = gtrends(c(“Trump”,“Biden”,“election”), time = “all”)\npar(family=“Georgia”)\nplot(BidenTrumpElection)\n\n\n\n\n\n\n\n\nDifferences\n\nAutomation\n\ngtrendsR allows you to automate the data retrieval process using R scripts. This can save you time and effort compared to manually visiting the Google Trends website each time.\n\nData Processing\n\nWith gtrendsR, you can process and analyze Google Trends data directly within R, making it easier to perform complex analyses, create visualizations, and combine Google Trends data with other datasets.\n\nCustomization\n\nWhile the Google Trends website offers customization options, gtrendsR provides finer control over data retrieval parameters, such as date ranges and geographic regions. You can also specify additional query parameters not available on the website.\nExample:\nOne keyword\nplot(gtrends(“transgender”, time = “all”))\n\nAdjust geographic region\nplot(gtrends(c(“transgender”), geo = c(“US”,“GB”,“TW”), time = “all”))\n\n\n\n\n\nMultiple Keywords\nplot(gtrends(c(“transgender”, “women right”, “feminism”), time = “all”))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Collection & Production",
    "section": "",
    "text": "Qualtrics\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPanel Data and Google Trends\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nText Data: Quanteda\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nYouTube Data: Word Cloud\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nWeb Scrapping\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nDownloding Multiple files\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nCensus Data using API\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nProject\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Assignment 1/index.html",
    "href": "posts/Assignment 1/index.html",
    "title": "Qualtrics",
    "section": "",
    "text": "✓Health information system - Medical history - Test results and medications ✓Library management systems - Availability of a book/article - Author’s information ✓Online shopping systems – Amazon and eBay - Price, details, and availability of a product"
  },
  {
    "objectID": "posts/Assignment 1/index.html#question-one",
    "href": "posts/Assignment 1/index.html#question-one",
    "title": "Qualtrics",
    "section": "",
    "text": "✓Health information system - Medical history - Test results and medications ✓Library management systems - Availability of a book/article - Author’s information ✓Online shopping systems – Amazon and eBay - Price, details, and availability of a product"
  },
  {
    "objectID": "posts/Assignment 1/index.html#question-two",
    "href": "posts/Assignment 1/index.html#question-two",
    "title": "Qualtrics",
    "section": "Question Two",
    "text": "Question Two\n\nPossible application in domain projects:\n✓Disease monitoring system: Track and control the spread of infectious disease. ✓Immunization system: Monitor vaccination of people globally. -Allow easy retrieval of immunization records -Allow individuals to update their immunization details ✓Publication archive: Store data relating to publications such as research papers, journals, books, and articles. - Act a central system for all publications"
  },
  {
    "objectID": "posts/Assignment 1/index.html#question-four",
    "href": "posts/Assignment 1/index.html#question-four",
    "title": "Qualtrics",
    "section": "Question Four",
    "text": "Question Four\n\n\n\n\n\n\n\nTraditional Databases\nNoSQL\n\n\n\n\nFollows the ACID properties\n\n\n\n(Atomic, consistent, isolated, durable)\nFollows CAP/BASE properties (Basically Available, Soft\n\n\nState, Eventually Consistent)\n\n\n\nTable-based\nColumn, graph, and document based\n\n\nFixed/predefined schema\nDynamic schema/Schema free\n\n\nSuitable for financial applications\nSuitable for big web applications like social media sites"
  },
  {
    "objectID": "posts/Assignment 1/index.html#question-five",
    "href": "posts/Assignment 1/index.html#question-five",
    "title": "Qualtrics",
    "section": "Question Five",
    "text": "Question Five\n\n\nTables used to store information in social media system\n✓ Profile table: Contains user’s information including account name, age, location, preferences, and official name. ✓Posts table: Contains content shared by a users including images, videos, and photos. ✓Analytics tables: Records statistics and data of content shared. For example, likes or comments on a post. ✓ Friends table: Contain information of friends’ or followers’ activities like posts shared"
  },
  {
    "objectID": "posts/Assignment 3/EPPS_6302_03.html",
    "href": "posts/Assignment 3/EPPS_6302_03.html",
    "title": "EPPS 6302: Assignment 3",
    "section": "",
    "text": "Quanteda\n\nPackages Required\n\n\nCode Here\n# Sample program for using quanteda for text modeling and analysis\n# Documentation: vignette(\"quickstart\", package = \"quanteda\")\n# Website: https://quanteda.io/\n\n#install.packages(c(\"readr\",\"quanteda\", \"quanteda.textmodels\", \"quanteda.textplots\",\"quanteda.textstats\",\"tidyverse\"))\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n\n\n\nExample with twitter data: A network plot is created using the “topgat_fcm” feature co-occurrence matrix\n\n\nCode Here\n# Twitter data about President Biden and Xi summit in Novemeber 2021\n# Do some background search/study on the event\n\nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\")\n\n#View(summit)\n#head(summit)\n# Extract text column from the dataset/df\nsum_twt = summit$text\n#class(sum_twt) # check data type\n# Tokenize the text\ntoks = tokens(sum_twt)\n#class(toks)\n#creates a document-feature matrix (DFM) from the tokens stored in the \"toks\" variable\nsumtwtdfm &lt;- dfm(toks)\n\n# Latent Semantic Analysis\n#textmodel_lsa is a function used to perform Latent Semantic Analysis (LSA) on a dfm \nsum_lsa &lt;- textmodel_lsa(sumtwtdfm)\n#summary(sum_lsa)\n#tokenize the text in the \"sum_twt\" variable while removing punctuation\ntweet_dfm &lt;- tokens(sum_twt, remove_punct = TRUE) %&gt;%\n  dfm() #pass the tokenized data to the dfm() function\n#head(tweet_dfm)\n#dfm_select is a function from the quanteda package used for subsetting a DFM based on a specified pattern\ntag_dfm &lt;- dfm_select(tweet_dfm, pattern = \"#*\") #only tokens that start with the '#' symbol\n#topfeatures is a function used to find the top N features (in this case, hashtags) in a DFM, based on their frequency.\ntoptag &lt;- names(topfeatures(tag_dfm, 50))\n#head(toptag, 10)\n# package generates various types of visualizations related to text data.\nlibrary(\"quanteda.textplots\")\n#fcm (feature co-occurrence matrix) is a function used to create a feature co-occurrence matrix from a DFM.\ntag_fcm &lt;- fcm(tag_dfm)\n#head(tag_fcm)\n#selects features (hashtags) that match the patterns stored in the \"toptag\" variable \ntopgat_fcm &lt;- fcm_select(tag_fcm, pattern = toptag)\n\n\n#a network plot is created using the \"topgat_fcm\" feature co-occurrence matrix\ntextplot_network(topgat_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 1)\n\n\n\n\n\n\n\nA network plot of top 50 mentions\n\n\nCode Here\n#\"tweet_dfm\" DFM is filtered to include only tokens that start with the '@' symbol, typically representing user mentions\nuser_dfm &lt;- dfm_select(tweet_dfm, pattern = \"@*\")\n#find the top 50 most frequently occurring user mentions\ntopuser &lt;- names(topfeatures(user_dfm, 50))\n#head(topuser, 20)\n#creates a feature co-occurrence matrix\nuser_fcm &lt;- fcm(user_dfm)\n#head(user_fcm, 20)\n\nuser_fcm &lt;- fcm_select(user_fcm, pattern = topuser)\n\ntextplot_network(user_fcm, min_freq = 20, edge_color = \"firebrick\", edge_alpha = 0.8, edge_size = 1)\n\n\n\n\n\n\n\nExample 2: Using presidential speeches\n\n\nCode Here\n# American Presidential speeches\n# Example extracted from https://quanteda.io/articles/pkgdown/examples/plotting.html\n\n#\"quanteda.textstats\" package, which contains functions and tools for text analysis and statistics using the Quanteda framework\nlibrary(quanteda.textstats)\n#corpus containing texts of U.S. presidential inaugural addresses.\ndata_corpus_inaugural_subset &lt;- \n  corpus_subset(data_corpus_inaugural, Year &gt; 1949) # create a subset \nkwic(tokens(data_corpus_inaugural_subset), pattern = \"american\") %&gt;%\n  textplot_xray()\n\n\n\n\n\n\n\nCode Here\ntextplot_xray(\n  kwic(data_corpus_inaugural_subset, pattern = \"american\"),\n  kwic(data_corpus_inaugural_subset, pattern = \"people\"),\n  kwic(data_corpus_inaugural_subset, pattern = \"communist\")\n)"
  },
  {
    "objectID": "posts/Assignment 6/EPPS_6302_06.html",
    "href": "posts/Assignment 6/EPPS_6302_06.html",
    "title": "Web Scrapping",
    "section": "",
    "text": "## Workshop: Scraping webpages with R rvest package\n# Prerequisites: Chrome browser, Selector Gadget\n#rm(list = ls())\n#install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.2\n\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n#install.packages(\"rvest\")\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nurl &lt;- 'https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves'\n#Reading the HTML code from the Wiki website\nwikiforreserve &lt;- read_html(url)\nclass(wikiforreserve)\n\n[1] \"xml_document\" \"xml_node\"    \n\n## Get the XPath data using Inspect element feature in Safari, Chrome or Firefox\n## At Inspect tab, look for &lt;table class=....&gt; tag. Leave the table close\n## Right click the table and Copy XPath, paste at html_nodes(xpath =)\n\nforeignreserve &lt;- wikiforreserve %&gt;%\n  html_nodes(xpath='//*[@id=\"mw-content-text\"]/div/table[1]') %&gt;%\n  html_table()\nclass(foreignreserve)\n\n[1] \"list\"\n\nfores = foreignreserve[[1]]\n\n\nnames(fores) &lt;- c(\"Rank\", \"Country\", \"Forexres\", \"Date\", \"Change\", \"Sources\")\ncolnames(fores)\n\n[1] \"Rank\"     \"Country\"  \"Forexres\" \"Date\"     \"Change\"   \"Sources\" \n\nhead(fores$Country, n=10)\n\n [1] \"China\"        \"Japan\"        \"Switzerland\"  \"India\"        \"Russia\"      \n [6] \"Taiwan\"       \"Saudi Arabia\" \"Hong Kong\"    \"South Korea\"  \"Brazil\"      \n\n## Clean up variables\n## What type is Rank?\n## How about Date?\n\n# Remove trailing notes in Date variable\nlibrary(stringr)\nfores$newdate = str_split_fixed(fores$Date, \"\\\\[\", n = 2)[, 1]\n\n#Get the csv file\nwrite.csv(fores, \"fores.csv\", row.names = FALSE)\n\n\n## Workshop: Scraping webpages with R rvest package\n## Prerequisites: Chrome browser, Selector Gadget\n\n# Load the tidyverse library for data manipulation and visualization\nlibrary(tidyverse)\n\n# Load the rvest package for web scraping\ninstall.packages(\"rvest\")\n\nWarning: package 'rvest' is in use and will not be installed\n\nlibrary(rvest)\n\n# Define the URL of the IMDb search results page for movies released between January 1, 2022, and January 1, 2023\nurl1 = \"https://www.imdb.com/search/title/?release_date=2022-01-01,2023-01-01\"\n\n# Read the HTML content of the IMDb page\nimdb2022 &lt;- read_html(url1)\n\n# Scrape movie ranks from the page (HTML nodes with class '.text-primary')\nrank_data_html &lt;- html_nodes(imdb2022, '.text-primary')\nrank_data &lt;- as.numeric(html_text(rank_data_html))\n\n# Display the first 10 movie ranks\nhead(rank_data, n = 10)\n\nnumeric(0)\n\n# Scrape movie titles from the page (HTML nodes with class '.lister-item-header a')\ntitle_data_html &lt;- html_nodes(imdb2022, '.lister-item-header a')\ntitle_data &lt;- html_text(title_data_html)\n\n# Display the first 20 movie titles\nhead(title_data, n = 20)\n\ncharacter(0)"
  },
  {
    "objectID": "posts/Assignment 8/index.html",
    "href": "posts/Assignment 8/index.html",
    "title": "Data Methods: Assignment 8",
    "section": "",
    "text": "Code\n#install.packages(c(\"tidyverse\", \"ggmap\",\"mapproj\", \"tidycensus\",\"tigris\"))\n#lapply(c(\"tidyverse\", \"ggmap\",\"mapproj\", \"tidycensus\",\"tigris\"), require, character.only = TRUE)\n\n# an API key is required to get Census data for map creation\n# Obtain the key at http://api.census.gov/data/key_signup.html\n# Enter information about organization and email address, then consent\n# Key will be provided to email, click on activate key (wait a few minutes to activate)\n# Store the key using the following function:\n#census_api_key(\"a270f2000ce58240ad0bbe712075fe5eced22fc7\", install = TRUE,overwrite = TRUE )\n# API key will be stored in  .Renviron and can be accessed by Sys.getenv(\"CENSUS_API_KEY\")\n\n\n# Substitute with your own Census API key\n\n#census_api_key(\"a270f2000ce58240ad0bbe712075fe5eced22fc7\", install = TRUE, overwrite = TRUE ) \n\nlibrary(tidycensus)\nlibrary(ggplot2)\nlibrary(tigris) # Load Census TIGER/Line Shapefiles\noptions(tigris_use_cache = TRUE)\n\n# Get a list of American Community Survey (ACS) 2010 variables\nacs10 = tidycensus::load_variables(2010, \"acs5\", cache = TRUE)\nacs10_Profile = load_variables(2010, \"acs5/profile\", cache = TRUE)\n\nus_median_age10 &lt;- get_acs(\n  geography = \"state\",\n  variables = \"B01002_001\",\n  year = 2010,\n  survey = \"acs1\",\n  geometry = TRUE,\n  resolution = \"20m\"\n) %&gt;%\n  shift_geometry()\n\n# plot(us_median_age10$geometry)\n\nggplot(data = us_median_age10, aes(fill = estimate)) + \n  geom_sf(col=\"white\") +  # Why color is white?\n  theme_bw() +\n  scale_fill_distiller(palette = \"PuBuGn\",  # Try other palette?\n                       direction = 1) + \n  labs(title = \"  Median Age by State, 2010\",\n       caption = \"Data source: 2020 1-year ACS, US Census Bureau\",\n       fill = \"\", family=\"Palatino\") +\n  theme(legend.position=c(.08,.6), legend.direction=\"vertical\") +\n  theme(text = element_text(family = \"Palatino\"), plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tidycensus)\nlibrary(tigris) # Load Census TIGER/Line Shapefiles\noptions(tigris_use_cache = TRUE)\n\n# Get a list of American Community Survey (ACS) 2020 variables\nacs20 = tidycensus::load_variables(2020, \"acs5\", cache = TRUE)\nacs20_Profile = load_variables(2020, \"acs5/profile\", cache = TRUE)\n\nus_median_age20 &lt;- get_acs(\n  geography = \"state\",\n  variables = \"B01002_001\",\n  year = 2019,\n  survey = \"acs1\",\n  geometry = TRUE,\n  resolution = \"20m\"\n) %&gt;%\n  shift_geometry()\n\n# plot(us_median_age10$geometry)\n\nggplot(data = us_median_age20, aes(fill = estimate)) + \n  geom_sf(col=\"white\") +  # Why color is white?\n  theme_bw() +\n  scale_fill_distiller(palette = \"PuBuGn\",  # Try other palette?\n                       direction = 1) + \n  labs(title = \"  Median Age by State, 2020\",\n       caption = \"Data source: 2020 1-year ACS, US Census Bureau\",\n       fill = \"\", family=\"Palatino\") +\n  theme(legend.position=c(.08,.6), legend.direction=\"vertical\") +\n  theme(text = element_text(family = \"Palatino\"), plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Collecting and mapping Census data using API: State data and maps\n#install.packages(c(\"tidyverse\", \"ggmap\",\"mapproj\", \"tidycensus\",\"tigris\", \"tmap\", \"mapview\"))\n#lapply(c(\"tidyverse\", \"ggmap\",\"mapproj\", \"tidycensus\",\"tigris\", \"tmap\", \"mapview\"), require, character.only = TRUE)\nlibrary(tidycensus)\noptions(tigris_use_cache = TRUE)\n\n\ntx_income &lt;- get_acs(\n  geography = \"tract\", \n  variables = \"B19013_001\",\n  state = \"TX\", \n  year = 2020,\n  geometry = TRUE\n)\n#tx_income - view data\nplot(tx_income[\"estimate\"])\n\n\n\n\n\n\n\n\n\n\nCode\n# Collecting and mapping Census data using API: State data and maps\n#install.packages(c(\"tidyverse\", \"ggmap\",\"mapproj\", \"tidycensus\",\"tigris\", \"tmap\", \"mapview\"))\n#lapply(c(\"tidyverse\", \"ggmap\",\"mapproj\", \"tidycensus\",\"tigris\", \"tmap\", \"mapview\"), require, character.only = TRUE)\nlibrary(tidycensus)\noptions(tigris_use_cache = TRUE)\n\n\ntx_income10 &lt;- get_acs(\n  geography = \"tract\", \n  variables = \"B19013_001\",\n  state = \"TX\", \n  year = 2010,\n  geometry = TRUE\n)\n#tx_income - view data\nplot(tx_income10[\"estimate\"])\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tmap)\ntmap_mode(\"view\")\n\ndallas_income &lt;- get_acs(\n  geography = \"tract\",\n  variables = \"B19013_001\",\n  year = 2020,\n  state = \"TX\",\n  county = \"Dallas\",\n  geometry = TRUE\n)\n\ntm_shape(dallas_income) + \n  tm_fill(col = \"estimate\", palette = \"YlOrRd\",\n          alpha = 0.5)\n\n\n\n\n\n\n\nCode\nlibrary(mapview)\nmapView(dallas_income, zcol = \"estimate\")\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tmap)\ntmap_mode(\"view\")\n\ndallas_income10 &lt;- get_acs(\n  geography = \"tract\",\n  variables = \"B19013_001\",\n  year = 2010,\n  state = \"TX\",\n  county = \"Dallas\",\n  geometry = TRUE\n)\n\ntm_shape(dallas_income10) + \n  tm_fill(col = \"estimate\", palette = \"YlOrRd\",\n          alpha = 0.5)\n\n\n\n\n\n\n\nCode\nlibrary(mapview)\nmapView(dallas_income10, zcol = \"estimate\")"
  },
  {
    "objectID": "posts/Assignment 8/index.html#collecting-and-mapping-census-data-using-api",
    "href": "posts/Assignment 8/index.html#collecting-and-mapping-census-data-using-api",
    "title": "Data Methods: Assignment 8",
    "section": "",
    "text": "Code\n#install.packages(c(\"tidyverse\", \"ggmap\",\"mapproj\", \"tidycensus\",\"tigris\"))\n#lapply(c(\"tidyverse\", \"ggmap\",\"mapproj\", \"tidycensus\",\"tigris\"), require, character.only = TRUE)\n\n# an API key is required to get Census data for map creation\n# Obtain the key at http://api.census.gov/data/key_signup.html\n# Enter information about organization and email address, then consent\n# Key will be provided to email, click on activate key (wait a few minutes to activate)\n# Store the key using the following function:\n#census_api_key(\"a270f2000ce58240ad0bbe712075fe5eced22fc7\", install = TRUE,overwrite = TRUE )\n# API key will be stored in  .Renviron and can be accessed by Sys.getenv(\"CENSUS_API_KEY\")\n\n\n# Substitute with your own Census API key\n\n#census_api_key(\"a270f2000ce58240ad0bbe712075fe5eced22fc7\", install = TRUE, overwrite = TRUE ) \n\nlibrary(tidycensus)\nlibrary(ggplot2)\nlibrary(tigris) # Load Census TIGER/Line Shapefiles\noptions(tigris_use_cache = TRUE)\n\n# Get a list of American Community Survey (ACS) 2010 variables\nacs10 = tidycensus::load_variables(2010, \"acs5\", cache = TRUE)\nacs10_Profile = load_variables(2010, \"acs5/profile\", cache = TRUE)\n\nus_median_age10 &lt;- get_acs(\n  geography = \"state\",\n  variables = \"B01002_001\",\n  year = 2010,\n  survey = \"acs1\",\n  geometry = TRUE,\n  resolution = \"20m\"\n) %&gt;%\n  shift_geometry()\n\n# plot(us_median_age10$geometry)\n\nggplot(data = us_median_age10, aes(fill = estimate)) + \n  geom_sf(col=\"white\") +  # Why color is white?\n  theme_bw() +\n  scale_fill_distiller(palette = \"PuBuGn\",  # Try other palette?\n                       direction = 1) + \n  labs(title = \"  Median Age by State, 2010\",\n       caption = \"Data source: 2020 1-year ACS, US Census Bureau\",\n       fill = \"\", family=\"Palatino\") +\n  theme(legend.position=c(.08,.6), legend.direction=\"vertical\") +\n  theme(text = element_text(family = \"Palatino\"), plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tidycensus)\nlibrary(tigris) # Load Census TIGER/Line Shapefiles\noptions(tigris_use_cache = TRUE)\n\n# Get a list of American Community Survey (ACS) 2020 variables\nacs20 = tidycensus::load_variables(2020, \"acs5\", cache = TRUE)\nacs20_Profile = load_variables(2020, \"acs5/profile\", cache = TRUE)\n\nus_median_age20 &lt;- get_acs(\n  geography = \"state\",\n  variables = \"B01002_001\",\n  year = 2019,\n  survey = \"acs1\",\n  geometry = TRUE,\n  resolution = \"20m\"\n) %&gt;%\n  shift_geometry()\n\n# plot(us_median_age10$geometry)\n\nggplot(data = us_median_age20, aes(fill = estimate)) + \n  geom_sf(col=\"white\") +  # Why color is white?\n  theme_bw() +\n  scale_fill_distiller(palette = \"PuBuGn\",  # Try other palette?\n                       direction = 1) + \n  labs(title = \"  Median Age by State, 2020\",\n       caption = \"Data source: 2020 1-year ACS, US Census Bureau\",\n       fill = \"\", family=\"Palatino\") +\n  theme(legend.position=c(.08,.6), legend.direction=\"vertical\") +\n  theme(text = element_text(family = \"Palatino\"), plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Collecting and mapping Census data using API: State data and maps\n#install.packages(c(\"tidyverse\", \"ggmap\",\"mapproj\", \"tidycensus\",\"tigris\", \"tmap\", \"mapview\"))\n#lapply(c(\"tidyverse\", \"ggmap\",\"mapproj\", \"tidycensus\",\"tigris\", \"tmap\", \"mapview\"), require, character.only = TRUE)\nlibrary(tidycensus)\noptions(tigris_use_cache = TRUE)\n\n\ntx_income &lt;- get_acs(\n  geography = \"tract\", \n  variables = \"B19013_001\",\n  state = \"TX\", \n  year = 2020,\n  geometry = TRUE\n)\n#tx_income - view data\nplot(tx_income[\"estimate\"])\n\n\n\n\n\n\n\n\n\n\nCode\n# Collecting and mapping Census data using API: State data and maps\n#install.packages(c(\"tidyverse\", \"ggmap\",\"mapproj\", \"tidycensus\",\"tigris\", \"tmap\", \"mapview\"))\n#lapply(c(\"tidyverse\", \"ggmap\",\"mapproj\", \"tidycensus\",\"tigris\", \"tmap\", \"mapview\"), require, character.only = TRUE)\nlibrary(tidycensus)\noptions(tigris_use_cache = TRUE)\n\n\ntx_income10 &lt;- get_acs(\n  geography = \"tract\", \n  variables = \"B19013_001\",\n  state = \"TX\", \n  year = 2010,\n  geometry = TRUE\n)\n#tx_income - view data\nplot(tx_income10[\"estimate\"])\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tmap)\ntmap_mode(\"view\")\n\ndallas_income &lt;- get_acs(\n  geography = \"tract\",\n  variables = \"B19013_001\",\n  year = 2020,\n  state = \"TX\",\n  county = \"Dallas\",\n  geometry = TRUE\n)\n\ntm_shape(dallas_income) + \n  tm_fill(col = \"estimate\", palette = \"YlOrRd\",\n          alpha = 0.5)\n\n\n\n\n\n\n\nCode\nlibrary(mapview)\nmapView(dallas_income, zcol = \"estimate\")\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tmap)\ntmap_mode(\"view\")\n\ndallas_income10 &lt;- get_acs(\n  geography = \"tract\",\n  variables = \"B19013_001\",\n  year = 2010,\n  state = \"TX\",\n  county = \"Dallas\",\n  geometry = TRUE\n)\n\ntm_shape(dallas_income10) + \n  tm_fill(col = \"estimate\", palette = \"YlOrRd\",\n          alpha = 0.5)\n\n\n\n\n\n\n\nCode\nlibrary(mapview)\nmapView(dallas_income10, zcol = \"estimate\")"
  }
]